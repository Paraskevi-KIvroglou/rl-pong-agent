{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Paraskevi-KIvroglou/rl-pong-agent/blob/main/Atari_Agent_Async.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kn5z7tAW5quJ",
        "outputId": "490096b1-e08f-45e6-933b-d02ca84c9a9b",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Collecting shimmy<1.0,>=0.1.0 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (6.4.0)\n",
            "Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ale-py, shimmy\n",
            "Successfully installed ale-py-0.8.1 shimmy-0.2.1\n",
            "Requirement already satisfied: gymnasium[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (0.0.4)\n",
            "Collecting autorom~=0.4.2 (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (4.66.4)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2024.7.4)\n",
            "Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446662 sha256=d222d2dfea1b36833876a87bf837bd166b7bd04e39aa1499443ae7ba3c5656a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.4.2\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.17.5-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.12.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.17.5-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.12.0-py2.py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.12.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.5\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install gymnasium\n",
        "!pip install gymnasium[atari]\n",
        "!pip install gymnasium[accept-rom-license]\n",
        "!pip install wandb\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6XZlIM6552C",
        "outputId": "e7cab138-f3a4-4550-fbed-01528573da4a",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py:90: SentryHubDeprecationWarning: `sentry_sdk.Hub` is deprecated and will be removed in a future major release. Please consult our 1.x to 2.x migration guide for details on how to migrate `Hub` usage to the new API: https://docs.sentry.io/platforms/python/migration/1.x-to-2.x\n",
            "  self.hub = sentry_sdk.Hub(client)\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "from torchvision import transforms\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "9gmNFByq59uC",
        "outputId": "c321e5ac-210c-4eb0-928c-b6d089ef1174",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  return LooseVersion(v) >= LooseVersion(check)\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240730_173931-01qoghxd</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/paraskevikivroglou/dqn-pong/runs/01qoghxd' target=\"_blank\">classic-meadow-24</a></strong> to <a href='https://wandb.ai/paraskevikivroglou/dqn-pong' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/paraskevikivroglou/dqn-pong' target=\"_blank\">https://wandb.ai/paraskevikivroglou/dqn-pong</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/paraskevikivroglou/dqn-pong/runs/01qoghxd' target=\"_blank\">https://wandb.ai/paraskevikivroglou/dqn-pong/runs/01qoghxd</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paraskevikivroglou/dqn-pong/runs/01qoghxd?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7acd0edd44c0>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.init(project=\"dqn-pong\", entity=\"paraskevikivroglou\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLP4gygD5_0P"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "        return self.fc(conv_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sh2EqOe56Dyr"
      },
      "outputs": [],
      "source": [
        "# Experience Replay buffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        experiences = random.sample(self.buffer, k=batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuITHTl26MTt"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_frame(frame):\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "    frame = cv2.resize(frame, (84, 84))\n",
        "    frame = frame / 255.0\n",
        "    return frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6S73NI3L6VNd"
      },
      "outputs": [],
      "source": [
        "# DQN Agent\n",
        "class DQNAgent:\n",
        "  def __init__(self, state_shape, n_actions, device):\n",
        "      self.device = device\n",
        "      self.dqn = DQN(state_shape, n_actions).to(device)\n",
        "      self.target_dqn = DQN(state_shape, n_actions).to(device)\n",
        "      self.target_dqn.load_state_dict(self.dqn.state_dict())\n",
        "      self.optimizer = optim.RMSprop(self.dqn.parameters(), lr=0.00025, alpha=0.95, eps=0.01)\n",
        "      self.memory = ReplayBuffer(1000000)\n",
        "      self.batch_size = 4\n",
        "      self.gamma = 0.99\n",
        "      self.epsilon = 1.0\n",
        "      self.epsilon_min = 0.1\n",
        "      self.epsilon_decay = 1000000\n",
        "      self.update_target_steps = 10000\n",
        "      self.steps = 0\n",
        "      self.rewards = []\n",
        "      self.losses = []\n",
        "\n",
        "  def remember(self, state, action, reward, next_state, done):\n",
        "      self.memory.push(state, action, reward, next_state, done)\n",
        "\n",
        "  def act(self, state):\n",
        "      if random.random() < self.epsilon:\n",
        "          return random.randrange(self.dqn.fc[-1].out_features)\n",
        "      with torch.no_grad():\n",
        "          state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "          q_values = self.dqn(state)\n",
        "          return q_values.argmax().item()\n",
        "\n",
        "  def update_target_network(self):\n",
        "      self.target_dqn.load_state_dict(self.dqn.state_dict())\n",
        "\n",
        "  def update_epsilon(self):\n",
        "      self.epsilon = max(self.epsilon_min, self.epsilon - (self.epsilon - self.epsilon_min) / self.epsilon_decay)\n",
        "\n",
        "  def train(self):\n",
        "      if len(self.memory) < self.batch_size:\n",
        "          return\n",
        "\n",
        "      batch = self.memory.sample(self.batch_size)\n",
        "      states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "      states = torch.FloatTensor(states).to(self.device)\n",
        "      actions = torch.LongTensor(actions).to(self.device)\n",
        "      rewards = torch.FloatTensor(rewards).to(self.device)\n",
        "      next_states = torch.FloatTensor(next_states).to(self.device)\n",
        "      dones = torch.FloatTensor(dones).to(self.device)\n",
        "\n",
        "      current_q_values = self.dqn(states).gather(1, actions.unsqueeze(1))\n",
        "      next_q_values = self.target_dqn(next_states).max(1)[0].detach()\n",
        "      target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
        "\n",
        "      loss = nn.functional.smooth_l1_loss(current_q_values, target_q_values.unsqueeze(1))\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      for param in self.dqn.parameters():\n",
        "          param.grad.data.clamp_(-1, 1)\n",
        "      self.optimizer.step()\n",
        "\n",
        "      self.steps += 1\n",
        "      if self.steps % self.update_target_steps == 0:\n",
        "          self.target_dqn.load_state_dict(self.dqn.state_dict())\n",
        "\n",
        "      self.update_epsilon()\n",
        "\n",
        "  def replay(self, batch_size):\n",
        "    memory = self.memory.buffer\n",
        "    minibatch = random.sample(memory, batch_size)\n",
        "    for state, action, reward, next_state, done in minibatch:\n",
        "        target = reward\n",
        "        self.rewards.append(reward)\n",
        "        if not done:\n",
        "            next_state = torch.FloatTensor(next_state).unsqueeze(0).to(self.device)\n",
        "            target += self.gamma * torch.max(self.target_dqn(next_state)).item()\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "        target_f = self.dqn(state)\n",
        "        target_f[0][action] = target\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = nn.MSELoss()(target_f, self.dqn(state))\n",
        "        loss.backward()\n",
        "        self.losses.append(loss.item())\n",
        "        self.optimizer.step()\n",
        "    if self.epsilon > self.epsilon_min:\n",
        "        self.epsilon *= self.epsilon_decay\n",
        "\n",
        "# Main training loop (assuming you have an environment)\n",
        "def train(env, agent, num_episodes):\n",
        "  for episode in range(num_episodes):\n",
        "      state = env.reset()\n",
        "      state = preprocess_frame(state[0])\n",
        "      state = np.stack([state] * 4, axis=0)\n",
        "      done = False\n",
        "      total_reward = 0\n",
        "\n",
        "      while not done:\n",
        "          action = agent.act(state)\n",
        "          next_state, reward, done, _ , _= env.step(action)\n",
        "          next_state = preprocess_frame(next_state)\n",
        "          next_state = np.append(state[1:], np.expand_dims(next_state, axis=0), axis=0)\n",
        "          agent.remember(state, action, reward, next_state, done)\n",
        "          state = next_state\n",
        "          total_reward += reward\n",
        "\n",
        "          if len(agent.memory) > 32:\n",
        "              loss = agent.replay(32)\n",
        "              agent.losses.append(loss)\n",
        "      print(episode)\n",
        "      agent.rewards.append(total_reward)\n",
        "      agent.update_target_network()\n",
        "\n",
        "      if episode % 10 == 0:\n",
        "          #print(f'Episode {e}/{1000}, Reward: {total_reward}, Epsilon: {agent.epsilon}')\n",
        "          wandb.log({\"episode\": episode, \"total_reward\": total_reward, \"Epsilon\": agent.epsilon})\n",
        "          print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5k_lBfS6V6v"
      },
      "outputs": [],
      "source": [
        "def worker(agent, env_name, max_episodes, epsilon, epsilon_decay, epsilon_min, global_rewards, global_losses):\n",
        "    env = gym.make(env_name)\n",
        "    for episode in range(max_episodes):\n",
        "        total_reward, loss = agent.train(env, max_steps=1000, epsilon=epsilon, epsilon_decay=epsilon_decay, epsilon_min=epsilon_min)\n",
        "        global_rewards.append(total_reward)\n",
        "        global_losses.append(loss)\n",
        "        if episode % 10 == 0:\n",
        "            print(f\"Thread {threading.current_thread().name}, Episode {episode}, Reward: {total_reward}, Loss: {loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nHnjFjAthHd"
      },
      "outputs": [],
      "source": [
        "thread_metrics = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtC8_stkUltf"
      },
      "outputs": [],
      "source": [
        "def check_and_initialize_optimizer_state(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        for param in param_group['params']:\n",
        "            state = optimizer.state[param]\n",
        "            if 'exp_avg' not in state:\n",
        "                state['exp_avg'] = torch.zeros_like(param.data)\n",
        "            if 'exp_avg_sq' not in state:\n",
        "                state['exp_avg_sq'] = torch.zeros_like(param.data)\n",
        "            if 'step' not in state:\n",
        "                state['step'] = torch.zeros(1, dtype=torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DbSSqvmo6qEc"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "\n",
        "def preprocess_state(state, device):\n",
        "    state = cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)\n",
        "    state = cv2.resize(state, (84, 84))\n",
        "    state = np.array(state, dtype=np.float32) / 255.0  # Normalize pixel values\n",
        "    state = np.expand_dims(state, axis=0)  # Add channel dimension\n",
        "    return torch.tensor(state, device=device)\n",
        "\n",
        "\n",
        "# Replay Buffer class\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        experiences = random.sample(self.buffer, k=batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "# Define the Q-network\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, input_shape, action_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        self.fc1 = nn.Linear(self.feature_size(input_shape), 512)\n",
        "        self.fc2 = nn.Linear(512, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def feature_size(self, input_shape):\n",
        "         with torch.no_grad():\n",
        "            return self.conv3(self.conv2(self.conv1(torch.zeros(1, *input_shape)))).view(1, -1).size(1)\n",
        "\n",
        "\n",
        "# Define the Agent\n",
        "class Agent:\n",
        "    def __init__(self, state_size, action_size, device, gamma=0.99):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.device = device\n",
        "        # self.optimizer = optimizer\n",
        "        self.gamma = gamma\n",
        "        #self.local_network.load_state_dict(self.global_network.state_dict())\n",
        "        self.q_network = QNetwork(state_size, action_size).to(device)\n",
        "        self.target_network = QNetwork(state_size, action_size).to(device)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001)\n",
        "\n",
        "        self.replay_buffer = ReplayBuffer(10000)\n",
        "        self.batch_size = 64\n",
        "        self.gamma = 0.99\n",
        "\n",
        "        self.epsilon = 0.5\n",
        "        self.epsilon_min = 0.1\n",
        "        self.decay_episodes = 200\n",
        "        self.epsilon_decay = (self.epsilon_min / self.epsilon) ** (1 / self.decay_episodes)\n",
        "        self.episode_count = 0\n",
        "\n",
        "        self.losses = []\n",
        "        self.q_values = []\n",
        "        self.epsilons = []\n",
        "        self.steps = []\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            action = random.choice(range(self.action_size))\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                q_values = self.q_network(state)\n",
        "                self.q_values.append(torch.max(q_values).item())\n",
        "                action = torch.argmax(q_values).item()\n",
        "        return action\n",
        "\n",
        "    def replay_experience(self):\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
        "        states = torch.stack([torch.tensor(s, dtype=torch.float32, device=self.device) for s in states])\n",
        "        next_states = torch.stack([torch.tensor(s, dtype=torch.float32, device=self.device) for s in next_states])\n",
        "        actions = torch.LongTensor(actions).to(self.device)\n",
        "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
        "        dones = torch.FloatTensor(dones).to(self.device)\n",
        "\n",
        "        q_values = self.q_network(states)\n",
        "        next_q_values = self.target_network(next_states)\n",
        "        q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "        next_q_value = next_q_values.max(1)[0]\n",
        "        expected_q_value = rewards + (1 - dones) * self.gamma * next_q_value\n",
        "\n",
        "        loss = nn.MSELoss()(q_value, expected_q_value)\n",
        "\n",
        "        check_and_initialize_optimizer_state(self.optimizer)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.losses.append(loss.item())\n",
        "\n",
        "    # def preprocess_state(self, state):\n",
        "    #     state = np.mean(state, axis=2).astype(np.float32)\n",
        "    #     state = np.resize(state, (84, 84))\n",
        "    #     state /= 255.0\n",
        "    #     return state\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "    def save_model(self, filename):\n",
        "        torch.save(self.q_network.state_dict(), filename)\n",
        "\n",
        "    def load_model(self, filename):\n",
        "        if os.path.isfile(filename):\n",
        "            self.q_network.load_state_dict(torch.load(filename))\n",
        "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "            print(f\"Model loaded from {filename}\")\n",
        "        else:\n",
        "            print(f\"No model file found at {filename}\")\n",
        "\n",
        "# Define the worker\n",
        "def worker(agent, env_name, max_episodes, lock, thread_id, thread_metrics):\n",
        "    env = gym.make(env_name)\n",
        "    total_steps = 0\n",
        "\n",
        "    thread_metrics[thread_id] = {\n",
        "        'epsilons': [],\n",
        "        'q_values': [],\n",
        "        'losses': [],\n",
        "        'steps' : []\n",
        "    }\n",
        "\n",
        "    for episode in range(max_episodes):\n",
        "        state = env.reset()\n",
        "        state = preprocess_state(state[0], agent.device)\n",
        "\n",
        "        state_stack = [state] * 4  # Create a stack of 4 initial frames .unsqueeze(0)\n",
        "        # state = state.repeat(1, 4, 1, 1)\n",
        "        #state = state.repeat(1, state.shape[0], 1, 1)\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "\n",
        "            state_tensor = torch.cat(state_stack, dim=0)\n",
        "            action = agent.choose_action(state_tensor.unsqueeze(0))\n",
        "            next_state, reward, done, _ , _= env.step(action)\n",
        "\n",
        "            next_state = preprocess_state(next_state, agent.device)\n",
        "            #next_state = next_state.unsqueeze(0).repeat(1, 1, 1, 1)\n",
        "            #next_state = np.append(state[1:], np.expand_dims(next_state, axis=0), axis=0)\n",
        "            state_stack.pop(0)  # Remove the oldest frame\n",
        "            state_stack.append(next_state)\n",
        "\n",
        "            next_state_tensor = torch.cat(state_stack, dim=0)  # Concatenate list of frames to tensor with shape (4, 84, 84)\n",
        "            agent.replay_buffer.push(state_tensor.cpu().numpy(), action, reward, next_state_tensor.cpu().numpy(), done)\n",
        "\n",
        "            total_reward += reward\n",
        "\n",
        "            with lock:\n",
        "                agent.replay_experience()\n",
        "\n",
        "            if total_steps % 100 == 0:\n",
        "                with lock:\n",
        "                    agent.update_target_network()\n",
        "\n",
        "            total_steps += 1\n",
        "\n",
        "        if episode % 10 == 0:\n",
        "            agent.epsilon = max(agent.epsilon_min, agent.epsilon * agent.epsilon_decay)\n",
        "        agent.episode_count += 1\n",
        "        thread_metrics[thread_id]['epsilons'].append(agent.epsilon)\n",
        "        thread_metrics[thread_id]['q_values'].extend(agent.q_values)\n",
        "        thread_metrics[thread_id]['losses'].extend(agent.losses)\n",
        "        thread_metrics[thread_id]['steps'].extend([total_steps] * len(agent.losses))\n",
        "        agent.q_values.clear()\n",
        "        agent.losses.clear()\n",
        "\n",
        "        print(f\"Thread: {thread_id}, Episode: {episode}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}\")\n",
        "\n",
        "\n",
        "# Main function to initialize agents and run threads\n",
        "def main(agent,env_name = 'PongNoFrameskip-v4', max_episodes = 300, load_model_file=None, save_model_file=None):\n",
        "    if load_model_file:\n",
        "        agent.load_model(load_model_file)\n",
        "\n",
        "    lock = threading.Lock()\n",
        "    threads = []\n",
        "    # thread_metrics = {}\n",
        "    num_threads = 4\n",
        "\n",
        "    for thread_id in range(num_threads):\n",
        "        t = threading.Thread(target=worker, args=(agent, env_name, max_episodes, lock, thread_id, thread_metrics))\n",
        "        t.start()\n",
        "        threads.append(t)\n",
        "\n",
        "    for t in threads:\n",
        "        t.join()\n",
        "\n",
        "    if save_model_file:\n",
        "        agent.save_model(save_model_file)\n",
        "\n",
        "    print(\"Training completed.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3SipS0QUg1G"
      },
      "outputs": [],
      "source": [
        "def plot_metrics(thread_metrics):\n",
        "    plt.figure(figsize=(15, 12))\n",
        "\n",
        "    for thread_id, metrics in thread_metrics.items():\n",
        "        plt.subplot(3, 1, 1)\n",
        "        plt.plot(metrics['epsilons'], label=f'Thread {thread_id}')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Epsilon')\n",
        "        plt.title('Epsilon Decay')\n",
        "\n",
        "        plt.subplot(3, 1, 2)\n",
        "        plt.plot(metrics['q_values'], label=f'Thread {thread_id}')\n",
        "        plt.xlabel('Step')\n",
        "        plt.ylabel('Q Value')\n",
        "        plt.title('Q Values')\n",
        "\n",
        "        plt.subplot(3, 1, 3)\n",
        "        plt.plot(metrics['losses'], label=f'Thread {thread_id}')\n",
        "        plt.xlabel('Step')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Losses')\n",
        "\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.legend()\n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.legend()\n",
        "    plt.subplot(3, 1, 3)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJLKFKgPF4Eu",
        "outputId": "333c9054-06fb-453b-9c55-9f026c9af3b9",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4, 84, 84)\n",
            "6\n",
            "Thread: 2, Episode: 0, Total Reward: -21.0, Epsilon: 0.49599255119493924\n",
            "Thread: 3, Episode: 0, Total Reward: -20.0, Epsilon: 0.49201722168172884\n",
            "Thread: 1, Episode: 0, Total Reward: -20.0, Epsilon: 0.48807375402753334\n",
            "Thread: 0, Episode: 0, Total Reward: -21.0, Epsilon: 0.484161892862815\n",
            "Thread: 2, Episode: 1, Total Reward: -20.0, Epsilon: 0.484161892862815\n",
            "Thread: 3, Episode: 1, Total Reward: -21.0, Epsilon: 0.484161892862815\n",
            "Thread: 0, Episode: 1, Total Reward: -21.0, Epsilon: 0.484161892862815\n",
            "Thread: 1, Episode: 1, Total Reward: -21.0, Epsilon: 0.484161892862815\n",
            "Thread: 2, Episode: 2, Total Reward: -21.0, Epsilon: 0.484161892862815\n",
            "Thread: 3, Episode: 2, Total Reward: -21.0, Epsilon: 0.484161892862815\n",
            "Thread: 0, Episode: 2, Total Reward: -21.0, Epsilon: 0.484161892862815\n",
            "Thread: 1, Episode: 2, Total Reward: -18.0, Epsilon: 0.484161892862815\n",
            "Thread: 2, Episode: 3, Total Reward: -21.0, Epsilon: 0.484161892862815\n",
            "Thread: 0, Episode: 3, Total Reward: -21.0, Epsilon: 0.484161892862815\n",
            "Thread: 3, Episode: 3, Total Reward: -20.0, Epsilon: 0.484161892862815\n",
            "Thread: 1, Episode: 3, Total Reward: -20.0, Epsilon: 0.484161892862815\n",
            "Thread: 2, Episode: 4, Total Reward: -21.0, Epsilon: 0.484161892862815\n",
            "Thread: 0, Episode: 4, Total Reward: -21.0, Epsilon: 0.484161892862815\n",
            "Thread: 3, Episode: 4, Total Reward: -21.0, Epsilon: 0.484161892862815\n",
            "Thread: 1, Episode: 4, Total Reward: -21.0, Epsilon: 0.484161892862815\n",
            "Thread: 3, Episode: 5, Total Reward: -20.0, Epsilon: 0.484161892862815\n",
            "Thread: 2, Episode: 5, Total Reward: -21.0, Epsilon: 0.484161892862815\n",
            "Thread: 0, Episode: 5, Total Reward: -21.0, Epsilon: 0.484161892862815\n",
            "Thread: 1, Episode: 5, Total Reward: -20.0, Epsilon: 0.484161892862815\n",
            "Thread: 3, Episode: 6, Total Reward: -20.0, Epsilon: 0.484161892862815\n",
            "Thread: 0, Episode: 6, Total Reward: -21.0, Epsilon: 0.484161892862815\n",
            "Thread: 2, Episode: 6, Total Reward: -20.0, Epsilon: 0.484161892862815\n",
            "Thread: 1, Episode: 6, Total Reward: -21.0, Epsilon: 0.484161892862815\n",
            "Thread: 3, Episode: 7, Total Reward: -21.0, Epsilon: 0.484161892862815\n",
            "Thread: 2, Episode: 7, Total Reward: -21.0, Epsilon: 0.484161892862815\n",
            "Thread: 0, Episode: 7, Total Reward: -20.0, Epsilon: 0.484161892862815\n",
            "Thread: 1, Episode: 7, Total Reward: -21.0, Epsilon: 0.484161892862815\n",
            "Thread: 3, Episode: 8, Total Reward: -21.0, Epsilon: 0.484161892862815\n",
            "Thread: 2, Episode: 8, Total Reward: -21.0, Epsilon: 0.484161892862815\n",
            "Thread: 0, Episode: 8, Total Reward: -20.0, Epsilon: 0.484161892862815\n",
            "Thread: 1, Episode: 8, Total Reward: -20.0, Epsilon: 0.484161892862815\n",
            "Thread: 0, Episode: 9, Total Reward: -21.0, Epsilon: 0.484161892862815\n",
            "Thread: 2, Episode: 9, Total Reward: -20.0, Epsilon: 0.484161892862815\n",
            "Thread: 3, Episode: 9, Total Reward: -20.0, Epsilon: 0.484161892862815\n",
            "Thread: 1, Episode: 9, Total Reward: -21.0, Epsilon: 0.484161892862815\n",
            "Thread: 0, Episode: 10, Total Reward: -21.0, Epsilon: 0.48028138486479693\n",
            "Thread: 1, Episode: 10, Total Reward: -21.0, Epsilon: 0.4764319787410582\n",
            "Thread: 2, Episode: 10, Total Reward: -21.0, Epsilon: 0.472613425213261\n",
            "Thread: 3, Episode: 10, Total Reward: -20.0, Epsilon: 0.4688254770010079\n",
            "Thread: 0, Episode: 11, Total Reward: -21.0, Epsilon: 0.4688254770010079\n",
            "Thread: 1, Episode: 11, Total Reward: -20.0, Epsilon: 0.4688254770010079\n",
            "Thread: 2, Episode: 11, Total Reward: -19.0, Epsilon: 0.4688254770010079\n",
            "Thread: 3, Episode: 11, Total Reward: -21.0, Epsilon: 0.4688254770010079\n",
            "Thread: 0, Episode: 12, Total Reward: -21.0, Epsilon: 0.4688254770010079\n",
            "Thread: 1, Episode: 12, Total Reward: -20.0, Epsilon: 0.4688254770010079\n",
            "Thread: 2, Episode: 12, Total Reward: -20.0, Epsilon: 0.4688254770010079\n",
            "Thread: 3, Episode: 12, Total Reward: -19.0, Epsilon: 0.4688254770010079\n",
            "Thread: 0, Episode: 13, Total Reward: -21.0, Epsilon: 0.4688254770010079\n",
            "Thread: 1, Episode: 13, Total Reward: -20.0, Epsilon: 0.4688254770010079\n",
            "Thread: 2, Episode: 13, Total Reward: -20.0, Epsilon: 0.4688254770010079\n",
            "Thread: 3, Episode: 13, Total Reward: -20.0, Epsilon: 0.4688254770010079\n",
            "Thread: 0, Episode: 14, Total Reward: -21.0, Epsilon: 0.4688254770010079\n",
            "Thread: 1, Episode: 14, Total Reward: -21.0, Epsilon: 0.4688254770010079\n",
            "Thread: 2, Episode: 14, Total Reward: -21.0, Epsilon: 0.4688254770010079\n",
            "Thread: 3, Episode: 14, Total Reward: -21.0, Epsilon: 0.4688254770010079\n",
            "Thread: 0, Episode: 15, Total Reward: -21.0, Epsilon: 0.4688254770010079\n",
            "Thread: 1, Episode: 15, Total Reward: -21.0, Epsilon: 0.4688254770010079\n",
            "Thread: 2, Episode: 15, Total Reward: -20.0, Epsilon: 0.4688254770010079\n",
            "Thread: 3, Episode: 15, Total Reward: -19.0, Epsilon: 0.4688254770010079\n",
            "Thread: 0, Episode: 16, Total Reward: -20.0, Epsilon: 0.4688254770010079\n",
            "Thread: 1, Episode: 16, Total Reward: -20.0, Epsilon: 0.4688254770010079\n",
            "Thread: 2, Episode: 16, Total Reward: -20.0, Epsilon: 0.4688254770010079\n",
            "Thread: 3, Episode: 16, Total Reward: -20.0, Epsilon: 0.4688254770010079\n",
            "Thread: 0, Episode: 17, Total Reward: -21.0, Epsilon: 0.4688254770010079\n",
            "Thread: 1, Episode: 17, Total Reward: -20.0, Epsilon: 0.4688254770010079\n",
            "Thread: 2, Episode: 17, Total Reward: -21.0, Epsilon: 0.4688254770010079\n",
            "Thread: 0, Episode: 18, Total Reward: -20.0, Epsilon: 0.4688254770010079\n",
            "Thread: 3, Episode: 17, Total Reward: -20.0, Epsilon: 0.4688254770010079\n",
            "Thread: 1, Episode: 18, Total Reward: -19.0, Epsilon: 0.4688254770010079\n",
            "Thread: 2, Episode: 18, Total Reward: -21.0, Epsilon: 0.4688254770010079\n",
            "Thread: 0, Episode: 19, Total Reward: -21.0, Epsilon: 0.4688254770010079\n",
            "Thread: 3, Episode: 18, Total Reward: -21.0, Epsilon: 0.4688254770010079\n",
            "Thread: 1, Episode: 19, Total Reward: -20.0, Epsilon: 0.4688254770010079\n",
            "Thread: 2, Episode: 19, Total Reward: -21.0, Epsilon: 0.4688254770010079\n"
          ]
        }
      ],
      "source": [
        "env_name = 'PongNoFrameskip-v4'\n",
        "state_size = (4, 84, 84)  # Example shape for Atari frames\n",
        "action_size = gym.make(env_name).action_space.n\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(state_size)\n",
        "print(action_size)\n",
        "\n",
        "agent = Agent(state_size, action_size, device)\n",
        "main(agent, save_model_file='dqn_model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iQ-fPXl3TY0"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "collapsed": true,
        "id": "qsbhBYhOyro2",
        "outputId": "a9340267-0014-4edd-d215-ba2aadf44b7f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'thread_metrics' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7f4fac07074f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'thread_metrics' is not defined"
          ]
        }
      ],
      "source": [
        "plot_metrics(thread_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eObUwj0kGtbJ"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(env_name, global_network, num_episodes=10):\n",
        "    env = gym.make(env_name)\n",
        "    total_rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        state = preprocess_frame(state)\n",
        "        state = np.stack([state] * 4, axis=0)\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            state = torch.FloatTensor(state).unsqueeze(0)\n",
        "            q_values = global_network(state)\n",
        "            action = torch.argmax(q_values).item()\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_state = preprocess_frame(next_state)\n",
        "            next_state = np.append(state[1:], np.expand_dims(next_state, axis=0), axis=0)\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        total_rewards"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}