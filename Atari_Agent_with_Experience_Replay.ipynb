{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPfLCaD+lNnrg7gvUzmexOu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Paraskevi-KIvroglou/rl-pong-agent/blob/main/Atari_Agent_with_Experience_Replay.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n",
        "!pip install gymnasium\n",
        "!pip install gymnasium[atari]\n",
        "!pip install gymnasium[accept-rom-license]\n",
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zwt-Y7nxIJ6",
        "outputId": "3cc42ebf-f580-49ae-f7b3-29c41b807ab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium[atari])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium[atari])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[atari]) (6.4.0)\n",
            "Installing collected packages: ale-py, shimmy\n",
            "Successfully installed ale-py-0.8.1 shimmy-0.2.1\n",
            "Requirement already satisfied: gymnasium[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (0.0.4)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2 (from gymnasium[accept-rom-license])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (4.66.4)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2024.6.2)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446661 sha256=f874c6957dd80a953926fa0b18814cf76dbfa307abaea573d155edcdc0836eb4\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.4.2\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.17.3-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.7.1-py2.py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.2/300.2 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.6.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.7.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeMm-k9DwPTI",
        "outputId": "9fdd5004-a4cb-4f6f-97f7-0d719494cdfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-331c0cb3e49c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mQNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, input_shape, action_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        self.fc1 = nn.Linear(self.feature_size(input_shape), 512)\n",
        "        self.fc2 = nn.Linear(512, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def feature_size(self, input_shape):\n",
        "        return self.conv3(self.conv2(self.conv1(torch.zeros(1, *input_shape)))).view(1, -1).size(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, input_shape, action_size):\n",
        "        self.input_shape = input_shape\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.99  # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.batch_size = 64\n",
        "        self.model = QNetwork(input_shape, action_size)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Grayscale(),\n",
        "            transforms.Resize((84, 84)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        self.epoch = 0\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def preprocess_frame(self, frame):\n",
        "        #print(type(frame))\n",
        "        frame = self.transform(frame)\n",
        "        frame_tensor = torch.tensor(frame, dtype=torch.float32) / 255.0\n",
        "        return frame_tensor #frame.numpy()\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.FloatTensor(state)\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        state = torch.FloatTensor(state)\n",
        "        #print(state.shape)\n",
        "        act_values = self.model(state)\n",
        "        #print(act_values.shape)\n",
        "        actions = torch.argmax(act_values, dim=1).tolist()\n",
        "        return actions[0]\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            state = torch.FloatTensor(state)\n",
        "            next_state = torch.FloatTensor(next_state)\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target += self.gamma * torch.max(self.model(next_state)).item()\n",
        "                #print(target)\n",
        "            target_f = self.model(state)\n",
        "            target_f = torch.transpose(target_f, 0, 1)\n",
        "            target_f[:][action] = target\n",
        "            self.optimizer.zero_grad()\n",
        "            target_f = torch.transpose(target_f, 0, 1)\n",
        "            loss = self.criterion(self.model(state), target_f)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            wandb.log({'epoch': self.epoch, 'loss': loss.item()})\n",
        "            self.epoch +=1\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_state_dict(torch.load(name))\n",
        "\n",
        "    def save(self, name):\n",
        "        torch.save(self.model.state_dict(), name)"
      ],
      "metadata": {
        "id": "83h_RyY3xmCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "from torchvision import transforms\n",
        "import wandb"
      ],
      "metadata": {
        "id": "yBiH-zhcxFtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"dqn-pong\", entity=\"paraskevikivroglou\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "iAZyps54Ltlt",
        "outputId": "c57709c2-d3c1-4564-c952-bc87ccaf4845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  return LooseVersion(v) >= LooseVersion(check)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240703_001005-1uopmdjj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/paraskevikivroglou/dqn-pong/runs/1uopmdjj' target=\"_blank\">vivid-cloud-19</a></strong> to <a href='https://wandb.ai/paraskevikivroglou/dqn-pong' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/paraskevikivroglou/dqn-pong' target=\"_blank\">https://wandb.ai/paraskevikivroglou/dqn-pong</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/paraskevikivroglou/dqn-pong/runs/1uopmdjj' target=\"_blank\">https://wandb.ai/paraskevikivroglou/dqn-pong/runs/1uopmdjj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paraskevikivroglou/dqn-pong/runs/1uopmdjj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x78770859e800>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "        return self.fc(conv_out)"
      ],
      "metadata": {
        "id": "jZCtmv6XSUPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experience Replay buffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        experiences = random.sample(self.buffer, k=batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "hoi4TD8US1lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_frame(frame):\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "    frame = cv2.resize(frame, (84, 84))\n",
        "    frame = frame / 255.0\n",
        "    return frame"
      ],
      "metadata": {
        "id": "XyJ-grjfiFfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DQN Agent\n",
        "class DQNAgent:\n",
        "  def __init__(self, state_shape, n_actions, device):\n",
        "      self.device = device\n",
        "      self.dqn = DQN(state_shape, n_actions).to(device)\n",
        "      self.target_dqn = DQN(state_shape, n_actions).to(device)\n",
        "      self.target_dqn.load_state_dict(self.dqn.state_dict())\n",
        "      self.optimizer = optim.RMSprop(self.dqn.parameters(), lr=0.00025, alpha=0.95, eps=0.01)\n",
        "      self.memory = ReplayBuffer(1000000)\n",
        "      self.batch_size = 4\n",
        "      self.gamma = 0.99\n",
        "      self.epsilon = 1.0\n",
        "      self.epsilon_min = 0.1\n",
        "      self.epsilon_decay = 1000000\n",
        "      self.update_target_steps = 10000\n",
        "      self.steps = 0\n",
        "      self.rewards = []\n",
        "      self.losses = []\n",
        "\n",
        "  def remember(self, state, action, reward, next_state, done):\n",
        "      self.memory.push(state, action, reward, next_state, done)\n",
        "\n",
        "  def act(self, state):\n",
        "      if random.random() < self.epsilon:\n",
        "          return random.randrange(self.dqn.fc[-1].out_features)\n",
        "      with torch.no_grad():\n",
        "          state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "          q_values = self.dqn(state)\n",
        "          return q_values.argmax().item()\n",
        "\n",
        "  def update_target_network(self):\n",
        "      self.target_dqn.load_state_dict(self.dqn.state_dict())\n",
        "\n",
        "  def update_epsilon(self):\n",
        "      self.epsilon = max(self.epsilon_min, self.epsilon - (self.epsilon - self.epsilon_min) / self.epsilon_decay)\n",
        "\n",
        "  def train(self):\n",
        "      if len(self.memory) < self.batch_size:\n",
        "          return\n",
        "\n",
        "      batch = self.memory.sample(self.batch_size)\n",
        "      states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "      states = torch.FloatTensor(states).to(self.device)\n",
        "      actions = torch.LongTensor(actions).to(self.device)\n",
        "      rewards = torch.FloatTensor(rewards).to(self.device)\n",
        "      next_states = torch.FloatTensor(next_states).to(self.device)\n",
        "      dones = torch.FloatTensor(dones).to(self.device)\n",
        "\n",
        "      current_q_values = self.dqn(states).gather(1, actions.unsqueeze(1))\n",
        "      next_q_values = self.target_dqn(next_states).max(1)[0].detach()\n",
        "      target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
        "\n",
        "      loss = nn.functional.smooth_l1_loss(current_q_values, target_q_values.unsqueeze(1))\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      for param in self.dqn.parameters():\n",
        "          param.grad.data.clamp_(-1, 1)\n",
        "      self.optimizer.step()\n",
        "\n",
        "      self.steps += 1\n",
        "      if self.steps % self.update_target_steps == 0:\n",
        "          self.target_dqn.load_state_dict(self.dqn.state_dict())\n",
        "\n",
        "      self.update_epsilon()\n",
        "\n",
        "  def replay(self, batch_size):\n",
        "    memory = self.memory.buffer\n",
        "    minibatch = random.sample(memory, batch_size)\n",
        "    for state, action, reward, next_state, done in minibatch:\n",
        "        target = reward\n",
        "        self.rewards.append(reward)\n",
        "        if not done:\n",
        "            next_state = torch.FloatTensor(next_state).unsqueeze(0).to(self.device)\n",
        "            target += self.gamma * torch.max(self.target_dqn(next_state)).item()\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "        target_f = self.dqn(state)\n",
        "        target_f[0][action] = target\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = nn.MSELoss()(target_f, self.dqn(state))\n",
        "        loss.backward()\n",
        "        self.losses.append(loss.item())\n",
        "        self.optimizer.step()\n",
        "    if self.epsilon > self.epsilon_min:\n",
        "        self.epsilon *= self.epsilon_decay\n",
        "\n",
        "# Main training loop (assuming you have an environment)\n",
        "def train(env, agent, num_episodes):\n",
        "  for episode in range(num_episodes):\n",
        "      state = env.reset()\n",
        "      state = preprocess_frame(state[0])\n",
        "      state = np.stack([state] * 4, axis=0)\n",
        "      done = False\n",
        "      total_reward = 0\n",
        "\n",
        "      while not done:\n",
        "          action = agent.act(state)\n",
        "          next_state, reward, done, _ , _= env.step(action)\n",
        "          next_state = preprocess_frame(next_state)\n",
        "          next_state = np.append(state[1:], np.expand_dims(next_state, axis=0), axis=0)\n",
        "          agent.remember(state, action, reward, next_state, done)\n",
        "          state = next_state\n",
        "          total_reward += reward\n",
        "\n",
        "          if len(agent.memory) > 32:\n",
        "              loss = agent.replay(32)\n",
        "              agent.losses.append(loss)\n",
        "      print(episode)\n",
        "      agent.rewards.append(total_reward)\n",
        "      agent.update_target_network()\n",
        "\n",
        "      if episode % 10 == 0:\n",
        "          #print(f'Episode {e}/{1000}, Reward: {total_reward}, Epsilon: {agent.epsilon}')\n",
        "          wandb.log({\"episode\": episode, \"total_reward\": total_reward, \"Epsilon\": agent.epsilon})\n",
        "          print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}\")"
      ],
      "metadata": {
        "id": "jP9pOR6PTTWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage (you need to set up your Atari environment)\n",
        "#import gym\n",
        "env = gym.make(\"PongNoFrameskip-v4\")\n",
        "state_shape = (4, 84, 84)  # Assuming you've preprocessed the frames\n",
        "n_actions = env.action_space.n\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "agent = DQNAgent(state_shape, n_actions, device)\n",
        "train(env, agent, num_episodes=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kFeJjwGTWQ6",
        "outputId": "f99cbee9-bbbd-464c-af7d-5aa33154c411",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Episode 0, Total Reward: -21.0, Epsilon: inf\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plotting results\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(agent.rewards)\n",
        "plt.title('Rewards per Episode')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(agent.losses)\n",
        "plt.title('Loss during Training')\n",
        "plt.xlabel('Training Steps')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "id": "uCR5hPZ4dfuZ",
        "outputId": "28eb6b13-95b5-463b-acf7-09d6c2986f37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'agent' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0f890350dc59>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Rewards per Episode'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Episode'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('ALE/Pong-v5')\n",
        "#render_mode='rgb_array',\n",
        "#frameskip=1,\n",
        "#repeat_action_probability=0.0,\n",
        "#full_action_space=False,\n",
        "#max_episode_steps=1000\n",
        "input_shape = (1, 84, 84)  # Example input shape (channels, height, width)\n",
        "action_size = env.action_space.n\n",
        "print(env.unwrapped.get_action_meanings())\n",
        "print(action_size)\n",
        "agent = DQNAgent(input_shape, action_size)\n",
        "done = False\n",
        "EPISODES = 1000\n",
        "reward_history = []\n",
        "\n",
        "for e in range(EPISODES):\n",
        "    init_state = env.reset()\n",
        "    total_reward = 0\n",
        "    state = agent.preprocess_frame(env.reset()[0])\n",
        "    state = np.stack([state] * 4, axis=0)  # Stack 4 frames\n",
        "    #print(state.shape)\n",
        "    for time in range(1000):\n",
        "        #print(state.shape)\n",
        "        action = agent.act(state)\n",
        "        #step = env.step(action)\n",
        "        #print(step)\n",
        "        next_frame, reward, done, truncated, info = env.step(action)\n",
        "        #print(next_frame.shape)\n",
        "        next_frame = agent.preprocess_frame(next_frame)\n",
        "        next_state = np.append(state[1:], [next_frame], axis=0)  # Append new frame to state\n",
        "        #print(next_state.shape)\n",
        "        total_reward += reward\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "        #print(next_state.shape)\n",
        "        state = next_state\n",
        "        #print(state.shape)\n",
        "        if done:\n",
        "            print(f\"episode: {e}/{EPISODES}, score: {time}, e: {agent.epsilon:.2}, Reward: {total_reward}\")\n",
        "            wandb.log({\"episode\": e/EPISODES, \"score\": time, \"e\": round(agent.epsilon,2), \"Total reward\" : total_reward})\n",
        "            reward_history.append(total_reward)\n",
        "            break\n",
        "    wandb.log({\"episode\": e/EPISODES, \"score\": time, \"e\": round(agent.epsilon,2), \"Total reward\" : total_reward})\n",
        "    agent.replay()\n",
        "    if e % 50 == 0:\n",
        "        agent.save(f\"dqn_model_{e}.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kzmyRRuQxrrt",
        "outputId": "345b6401-21ba-4fc1-8786-8747e51b2769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
            "6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-3a39b8440924>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  frame_tensor = torch.tensor(frame, dtype=torch.float32) / 255.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 0/1000, score: 763, e: 1.0, Reward: -21.0\n",
            "episode: 1/1000, score: 823, e: 0.99, Reward: -21.0\n",
            "episode: 2/1000, score: 791, e: 0.99, Reward: -21.0\n",
            "episode: 3/1000, score: 791, e: 0.99, Reward: -21.0\n",
            "episode: 4/1000, score: 901, e: 0.98, Reward: -20.0\n",
            "episode: 5/1000, score: 888, e: 0.98, Reward: -20.0\n",
            "episode: 7/1000, score: 967, e: 0.97, Reward: -20.0\n",
            "episode: 8/1000, score: 946, e: 0.96, Reward: -21.0\n",
            "episode: 9/1000, score: 810, e: 0.96, Reward: -21.0\n",
            "episode: 11/1000, score: 823, e: 0.95, Reward: -21.0\n",
            "episode: 12/1000, score: 763, e: 0.94, Reward: -21.0\n",
            "episode: 13/1000, score: 941, e: 0.94, Reward: -21.0\n",
            "episode: 14/1000, score: 947, e: 0.93, Reward: -20.0\n",
            "episode: 18/1000, score: 956, e: 0.91, Reward: -20.0\n",
            "episode: 19/1000, score: 841, e: 0.91, Reward: -20.0\n",
            "episode: 20/1000, score: 930, e: 0.9, Reward: -20.0\n",
            "episode: 21/1000, score: 950, e: 0.9, Reward: -20.0\n",
            "episode: 23/1000, score: 962, e: 0.89, Reward: -20.0\n",
            "episode: 24/1000, score: 913, e: 0.89, Reward: -21.0\n",
            "episode: 25/1000, score: 823, e: 0.88, Reward: -21.0\n",
            "episode: 26/1000, score: 851, e: 0.88, Reward: -21.0\n",
            "episode: 27/1000, score: 763, e: 0.87, Reward: -21.0\n",
            "episode: 28/1000, score: 763, e: 0.87, Reward: -21.0\n",
            "episode: 29/1000, score: 791, e: 0.86, Reward: -21.0\n",
            "episode: 30/1000, score: 782, e: 0.86, Reward: -21.0\n",
            "episode: 31/1000, score: 866, e: 0.86, Reward: -20.0\n",
            "episode: 32/1000, score: 979, e: 0.85, Reward: -19.0\n",
            "episode: 33/1000, score: 837, e: 0.85, Reward: -20.0\n",
            "episode: 34/1000, score: 971, e: 0.84, Reward: -19.0\n",
            "episode: 35/1000, score: 763, e: 0.84, Reward: -21.0\n",
            "episode: 36/1000, score: 979, e: 0.83, Reward: -20.0\n",
            "episode: 37/1000, score: 971, e: 0.83, Reward: -20.0\n",
            "episode: 38/1000, score: 782, e: 0.83, Reward: -21.0\n",
            "episode: 39/1000, score: 825, e: 0.82, Reward: -21.0\n",
            "episode: 40/1000, score: 763, e: 0.82, Reward: -21.0\n",
            "episode: 41/1000, score: 823, e: 0.81, Reward: -21.0\n",
            "episode: 42/1000, score: 841, e: 0.81, Reward: -20.0\n",
            "episode: 43/1000, score: 886, e: 0.81, Reward: -20.0\n",
            "episode: 44/1000, score: 763, e: 0.8, Reward: -21.0\n",
            "episode: 45/1000, score: 824, e: 0.8, Reward: -21.0\n",
            "episode: 46/1000, score: 927, e: 0.79, Reward: -21.0\n",
            "episode: 47/1000, score: 763, e: 0.79, Reward: -21.0\n",
            "episode: 48/1000, score: 943, e: 0.79, Reward: -21.0\n",
            "episode: 49/1000, score: 919, e: 0.78, Reward: -20.0\n",
            "episode: 50/1000, score: 763, e: 0.78, Reward: -21.0\n",
            "episode: 51/1000, score: 763, e: 0.77, Reward: -21.0\n",
            "episode: 52/1000, score: 922, e: 0.77, Reward: -20.0\n",
            "episode: 53/1000, score: 858, e: 0.77, Reward: -20.0\n",
            "episode: 54/1000, score: 837, e: 0.76, Reward: -20.0\n",
            "episode: 56/1000, score: 791, e: 0.76, Reward: -21.0\n",
            "episode: 58/1000, score: 997, e: 0.75, Reward: -19.0\n",
            "episode: 59/1000, score: 939, e: 0.74, Reward: -21.0\n",
            "episode: 60/1000, score: 881, e: 0.74, Reward: -21.0\n",
            "episode: 61/1000, score: 947, e: 0.74, Reward: -19.0\n",
            "episode: 62/1000, score: 763, e: 0.73, Reward: -21.0\n",
            "episode: 63/1000, score: 884, e: 0.73, Reward: -21.0\n",
            "episode: 64/1000, score: 980, e: 0.73, Reward: -20.0\n",
            "episode: 65/1000, score: 948, e: 0.72, Reward: -20.0\n",
            "episode: 66/1000, score: 763, e: 0.72, Reward: -21.0\n",
            "episode: 67/1000, score: 841, e: 0.71, Reward: -20.0\n",
            "episode: 68/1000, score: 763, e: 0.71, Reward: -21.0\n",
            "episode: 69/1000, score: 823, e: 0.71, Reward: -21.0\n",
            "episode: 70/1000, score: 824, e: 0.7, Reward: -21.0\n",
            "episode: 71/1000, score: 884, e: 0.7, Reward: -21.0\n",
            "episode: 72/1000, score: 782, e: 0.7, Reward: -21.0\n",
            "episode: 73/1000, score: 763, e: 0.69, Reward: -21.0\n",
            "episode: 74/1000, score: 836, e: 0.69, Reward: -20.0\n",
            "episode: 75/1000, score: 902, e: 0.69, Reward: -21.0\n",
            "episode: 76/1000, score: 819, e: 0.68, Reward: -21.0\n",
            "episode: 77/1000, score: 825, e: 0.68, Reward: -21.0\n",
            "episode: 78/1000, score: 763, e: 0.68, Reward: -21.0\n",
            "episode: 79/1000, score: 823, e: 0.67, Reward: -21.0\n",
            "episode: 80/1000, score: 763, e: 0.67, Reward: -21.0\n",
            "episode: 81/1000, score: 763, e: 0.67, Reward: -21.0\n",
            "episode: 82/1000, score: 763, e: 0.66, Reward: -21.0\n",
            "episode: 83/1000, score: 781, e: 0.66, Reward: -21.0\n",
            "episode: 85/1000, score: 763, e: 0.65, Reward: -21.0\n",
            "episode: 86/1000, score: 763, e: 0.65, Reward: -21.0\n",
            "episode: 87/1000, score: 841, e: 0.65, Reward: -20.0\n",
            "episode: 88/1000, score: 763, e: 0.64, Reward: -21.0\n",
            "episode: 89/1000, score: 763, e: 0.64, Reward: -21.0\n",
            "episode: 90/1000, score: 793, e: 0.64, Reward: -21.0\n",
            "episode: 91/1000, score: 842, e: 0.63, Reward: -20.0\n",
            "episode: 92/1000, score: 889, e: 0.63, Reward: -20.0\n",
            "episode: 93/1000, score: 763, e: 0.63, Reward: -21.0\n",
            "episode: 94/1000, score: 782, e: 0.62, Reward: -21.0\n",
            "episode: 95/1000, score: 791, e: 0.62, Reward: -21.0\n",
            "episode: 96/1000, score: 823, e: 0.62, Reward: -21.0\n",
            "episode: 97/1000, score: 763, e: 0.61, Reward: -21.0\n",
            "episode: 98/1000, score: 763, e: 0.61, Reward: -21.0\n",
            "episode: 99/1000, score: 975, e: 0.61, Reward: -21.0\n",
            "episode: 100/1000, score: 801, e: 0.61, Reward: -21.0\n",
            "episode: 101/1000, score: 899, e: 0.6, Reward: -20.0\n",
            "episode: 102/1000, score: 763, e: 0.6, Reward: -21.0\n",
            "episode: 103/1000, score: 763, e: 0.6, Reward: -21.0\n",
            "episode: 104/1000, score: 887, e: 0.59, Reward: -21.0\n",
            "episode: 105/1000, score: 763, e: 0.59, Reward: -21.0\n",
            "episode: 106/1000, score: 791, e: 0.59, Reward: -21.0\n",
            "episode: 107/1000, score: 819, e: 0.58, Reward: -21.0\n",
            "episode: 108/1000, score: 763, e: 0.58, Reward: -21.0\n",
            "episode: 109/1000, score: 911, e: 0.58, Reward: -21.0\n",
            "episode: 110/1000, score: 931, e: 0.58, Reward: -21.0\n",
            "episode: 111/1000, score: 782, e: 0.57, Reward: -21.0\n",
            "episode: 112/1000, score: 823, e: 0.57, Reward: -21.0\n",
            "episode: 113/1000, score: 841, e: 0.57, Reward: -20.0\n",
            "episode: 114/1000, score: 763, e: 0.56, Reward: -21.0\n",
            "episode: 115/1000, score: 994, e: 0.56, Reward: -19.0\n",
            "episode: 116/1000, score: 975, e: 0.56, Reward: -21.0\n",
            "episode: 117/1000, score: 851, e: 0.56, Reward: -21.0\n",
            "episode: 118/1000, score: 763, e: 0.55, Reward: -21.0\n",
            "episode: 119/1000, score: 944, e: 0.55, Reward: -21.0\n",
            "episode: 120/1000, score: 957, e: 0.55, Reward: -20.0\n",
            "episode: 121/1000, score: 763, e: 0.55, Reward: -21.0\n",
            "episode: 122/1000, score: 782, e: 0.54, Reward: -21.0\n",
            "episode: 123/1000, score: 763, e: 0.54, Reward: -21.0\n",
            "episode: 124/1000, score: 961, e: 0.54, Reward: -20.0\n",
            "episode: 125/1000, score: 870, e: 0.53, Reward: -21.0\n",
            "episode: 126/1000, score: 823, e: 0.53, Reward: -21.0\n",
            "episode: 127/1000, score: 763, e: 0.53, Reward: -21.0\n",
            "episode: 128/1000, score: 763, e: 0.53, Reward: -21.0\n",
            "episode: 129/1000, score: 870, e: 0.52, Reward: -21.0\n",
            "episode: 130/1000, score: 885, e: 0.52, Reward: -21.0\n",
            "episode: 131/1000, score: 943, e: 0.52, Reward: -21.0\n",
            "episode: 132/1000, score: 763, e: 0.52, Reward: -21.0\n",
            "episode: 133/1000, score: 782, e: 0.51, Reward: -21.0\n",
            "episode: 134/1000, score: 763, e: 0.51, Reward: -21.0\n",
            "episode: 135/1000, score: 763, e: 0.51, Reward: -21.0\n",
            "episode: 136/1000, score: 851, e: 0.51, Reward: -21.0\n",
            "episode: 137/1000, score: 885, e: 0.5, Reward: -21.0\n",
            "episode: 138/1000, score: 763, e: 0.5, Reward: -21.0\n",
            "episode: 139/1000, score: 823, e: 0.5, Reward: -21.0\n",
            "episode: 140/1000, score: 823, e: 0.5, Reward: -21.0\n",
            "episode: 141/1000, score: 763, e: 0.49, Reward: -21.0\n",
            "episode: 142/1000, score: 819, e: 0.49, Reward: -21.0\n",
            "episode: 143/1000, score: 810, e: 0.49, Reward: -21.0\n",
            "episode: 144/1000, score: 864, e: 0.49, Reward: -20.0\n",
            "episode: 145/1000, score: 763, e: 0.48, Reward: -21.0\n",
            "episode: 146/1000, score: 823, e: 0.48, Reward: -21.0\n",
            "episode: 148/1000, score: 763, e: 0.48, Reward: -21.0\n",
            "episode: 149/1000, score: 763, e: 0.47, Reward: -21.0\n",
            "episode: 150/1000, score: 907, e: 0.47, Reward: -21.0\n",
            "episode: 151/1000, score: 973, e: 0.47, Reward: -21.0\n",
            "episode: 152/1000, score: 884, e: 0.47, Reward: -21.0\n",
            "episode: 153/1000, score: 851, e: 0.46, Reward: -21.0\n",
            "episode: 154/1000, score: 763, e: 0.46, Reward: -21.0\n",
            "episode: 155/1000, score: 763, e: 0.46, Reward: -21.0\n",
            "episode: 156/1000, score: 885, e: 0.46, Reward: -21.0\n",
            "episode: 157/1000, score: 897, e: 0.46, Reward: -20.0\n",
            "episode: 158/1000, score: 851, e: 0.45, Reward: -21.0\n",
            "episode: 159/1000, score: 930, e: 0.45, Reward: -21.0\n",
            "episode: 160/1000, score: 763, e: 0.45, Reward: -21.0\n",
            "episode: 161/1000, score: 782, e: 0.45, Reward: -21.0\n",
            "episode: 162/1000, score: 825, e: 0.44, Reward: -21.0\n",
            "episode: 163/1000, score: 763, e: 0.44, Reward: -21.0\n",
            "episode: 164/1000, score: 900, e: 0.44, Reward: -21.0\n",
            "episode: 165/1000, score: 763, e: 0.44, Reward: -21.0\n",
            "episode: 166/1000, score: 763, e: 0.44, Reward: -21.0\n",
            "episode: 167/1000, score: 883, e: 0.43, Reward: -21.0\n",
            "episode: 168/1000, score: 885, e: 0.43, Reward: -21.0\n",
            "episode: 169/1000, score: 823, e: 0.43, Reward: -21.0\n",
            "episode: 170/1000, score: 782, e: 0.43, Reward: -21.0\n",
            "episode: 171/1000, score: 791, e: 0.42, Reward: -21.0\n",
            "episode: 172/1000, score: 763, e: 0.42, Reward: -21.0\n",
            "episode: 173/1000, score: 763, e: 0.42, Reward: -21.0\n",
            "episode: 174/1000, score: 791, e: 0.42, Reward: -21.0\n",
            "episode: 175/1000, score: 763, e: 0.42, Reward: -21.0\n",
            "episode: 176/1000, score: 929, e: 0.41, Reward: -20.0\n",
            "episode: 177/1000, score: 763, e: 0.41, Reward: -21.0\n",
            "episode: 178/1000, score: 959, e: 0.41, Reward: -20.0\n",
            "episode: 179/1000, score: 913, e: 0.41, Reward: -21.0\n",
            "episode: 180/1000, score: 763, e: 0.41, Reward: -21.0\n",
            "episode: 181/1000, score: 843, e: 0.4, Reward: -20.0\n",
            "episode: 182/1000, score: 763, e: 0.4, Reward: -21.0\n",
            "episode: 183/1000, score: 885, e: 0.4, Reward: -21.0\n",
            "episode: 184/1000, score: 843, e: 0.4, Reward: -20.0\n",
            "episode: 185/1000, score: 898, e: 0.4, Reward: -20.0\n",
            "episode: 186/1000, score: 763, e: 0.39, Reward: -21.0\n",
            "episode: 187/1000, score: 791, e: 0.39, Reward: -21.0\n",
            "episode: 188/1000, score: 823, e: 0.39, Reward: -21.0\n",
            "episode: 189/1000, score: 763, e: 0.39, Reward: -21.0\n",
            "episode: 190/1000, score: 763, e: 0.39, Reward: -21.0\n",
            "episode: 191/1000, score: 978, e: 0.38, Reward: -20.0\n",
            "episode: 192/1000, score: 865, e: 0.38, Reward: -20.0\n",
            "episode: 193/1000, score: 837, e: 0.38, Reward: -20.0\n",
            "episode: 194/1000, score: 825, e: 0.38, Reward: -21.0\n",
            "episode: 195/1000, score: 841, e: 0.38, Reward: -20.0\n",
            "episode: 196/1000, score: 763, e: 0.37, Reward: -21.0\n",
            "episode: 197/1000, score: 763, e: 0.37, Reward: -21.0\n",
            "episode: 198/1000, score: 793, e: 0.37, Reward: -21.0\n",
            "episode: 199/1000, score: 991, e: 0.37, Reward: -20.0\n",
            "episode: 201/1000, score: 823, e: 0.37, Reward: -21.0\n",
            "episode: 202/1000, score: 763, e: 0.36, Reward: -21.0\n",
            "episode: 203/1000, score: 763, e: 0.36, Reward: -21.0\n",
            "episode: 204/1000, score: 825, e: 0.36, Reward: -21.0\n",
            "episode: 205/1000, score: 844, e: 0.36, Reward: -21.0\n",
            "episode: 206/1000, score: 877, e: 0.36, Reward: -20.0\n",
            "episode: 207/1000, score: 825, e: 0.35, Reward: -21.0\n",
            "episode: 208/1000, score: 763, e: 0.35, Reward: -21.0\n",
            "episode: 209/1000, score: 763, e: 0.35, Reward: -21.0\n",
            "episode: 210/1000, score: 763, e: 0.35, Reward: -21.0\n",
            "episode: 211/1000, score: 782, e: 0.35, Reward: -21.0\n",
            "episode: 212/1000, score: 763, e: 0.35, Reward: -21.0\n",
            "episode: 213/1000, score: 837, e: 0.34, Reward: -20.0\n",
            "episode: 214/1000, score: 763, e: 0.34, Reward: -21.0\n",
            "episode: 215/1000, score: 855, e: 0.34, Reward: -21.0\n",
            "episode: 216/1000, score: 763, e: 0.34, Reward: -21.0\n",
            "episode: 217/1000, score: 763, e: 0.34, Reward: -21.0\n",
            "episode: 218/1000, score: 871, e: 0.34, Reward: -21.0\n",
            "episode: 219/1000, score: 869, e: 0.33, Reward: -20.0\n",
            "episode: 220/1000, score: 782, e: 0.33, Reward: -21.0\n",
            "episode: 221/1000, score: 763, e: 0.33, Reward: -21.0\n",
            "episode: 222/1000, score: 823, e: 0.33, Reward: -21.0\n",
            "episode: 223/1000, score: 763, e: 0.33, Reward: -21.0\n",
            "episode: 224/1000, score: 851, e: 0.33, Reward: -21.0\n",
            "episode: 225/1000, score: 763, e: 0.32, Reward: -21.0\n",
            "episode: 226/1000, score: 825, e: 0.32, Reward: -21.0\n",
            "episode: 227/1000, score: 883, e: 0.32, Reward: -21.0\n",
            "episode: 228/1000, score: 763, e: 0.32, Reward: -21.0\n",
            "episode: 229/1000, score: 763, e: 0.32, Reward: -21.0\n",
            "episode: 230/1000, score: 763, e: 0.32, Reward: -21.0\n",
            "episode: 231/1000, score: 824, e: 0.31, Reward: -21.0\n",
            "episode: 232/1000, score: 825, e: 0.31, Reward: -21.0\n",
            "episode: 233/1000, score: 869, e: 0.31, Reward: -20.0\n",
            "episode: 234/1000, score: 902, e: 0.31, Reward: -21.0\n",
            "episode: 235/1000, score: 763, e: 0.31, Reward: -21.0\n",
            "episode: 236/1000, score: 763, e: 0.31, Reward: -21.0\n",
            "episode: 237/1000, score: 763, e: 0.3, Reward: -21.0\n",
            "episode: 238/1000, score: 899, e: 0.3, Reward: -20.0\n",
            "episode: 240/1000, score: 823, e: 0.3, Reward: -21.0\n",
            "episode: 242/1000, score: 763, e: 0.3, Reward: -21.0\n",
            "episode: 243/1000, score: 825, e: 0.3, Reward: -21.0\n",
            "episode: 244/1000, score: 791, e: 0.29, Reward: -21.0\n",
            "episode: 245/1000, score: 763, e: 0.29, Reward: -21.0\n",
            "episode: 246/1000, score: 763, e: 0.29, Reward: -21.0\n",
            "episode: 248/1000, score: 763, e: 0.29, Reward: -21.0\n",
            "episode: 249/1000, score: 823, e: 0.29, Reward: -21.0\n",
            "episode: 250/1000, score: 763, e: 0.29, Reward: -21.0\n",
            "episode: 251/1000, score: 869, e: 0.28, Reward: -20.0\n",
            "episode: 252/1000, score: 763, e: 0.28, Reward: -21.0\n",
            "episode: 253/1000, score: 823, e: 0.28, Reward: -21.0\n",
            "episode: 254/1000, score: 823, e: 0.28, Reward: -21.0\n",
            "episode: 255/1000, score: 918, e: 0.28, Reward: -20.0\n",
            "episode: 256/1000, score: 978, e: 0.28, Reward: -20.0\n",
            "episode: 257/1000, score: 869, e: 0.28, Reward: -20.0\n",
            "episode: 258/1000, score: 763, e: 0.27, Reward: -21.0\n",
            "episode: 259/1000, score: 763, e: 0.27, Reward: -21.0\n",
            "episode: 260/1000, score: 913, e: 0.27, Reward: -21.0\n",
            "episode: 261/1000, score: 920, e: 0.27, Reward: -20.0\n",
            "episode: 262/1000, score: 864, e: 0.27, Reward: -20.0\n",
            "episode: 263/1000, score: 763, e: 0.27, Reward: -21.0\n",
            "episode: 264/1000, score: 763, e: 0.27, Reward: -21.0\n",
            "episode: 265/1000, score: 844, e: 0.26, Reward: -21.0\n",
            "episode: 266/1000, score: 782, e: 0.26, Reward: -21.0\n",
            "episode: 267/1000, score: 763, e: 0.26, Reward: -21.0\n",
            "episode: 268/1000, score: 763, e: 0.26, Reward: -21.0\n",
            "episode: 269/1000, score: 791, e: 0.26, Reward: -21.0\n",
            "episode: 270/1000, score: 954, e: 0.26, Reward: -20.0\n",
            "episode: 271/1000, score: 763, e: 0.26, Reward: -21.0\n",
            "episode: 272/1000, score: 763, e: 0.26, Reward: -21.0\n",
            "episode: 273/1000, score: 823, e: 0.25, Reward: -21.0\n",
            "episode: 274/1000, score: 865, e: 0.25, Reward: -20.0\n",
            "episode: 275/1000, score: 911, e: 0.25, Reward: -21.0\n",
            "episode: 276/1000, score: 763, e: 0.25, Reward: -21.0\n",
            "episode: 277/1000, score: 763, e: 0.25, Reward: -21.0\n",
            "episode: 278/1000, score: 825, e: 0.25, Reward: -21.0\n",
            "episode: 279/1000, score: 763, e: 0.25, Reward: -21.0\n",
            "episode: 280/1000, score: 823, e: 0.25, Reward: -21.0\n",
            "episode: 281/1000, score: 763, e: 0.24, Reward: -21.0\n",
            "episode: 282/1000, score: 823, e: 0.24, Reward: -21.0\n",
            "episode: 283/1000, score: 841, e: 0.24, Reward: -20.0\n",
            "episode: 284/1000, score: 825, e: 0.24, Reward: -21.0\n",
            "episode: 285/1000, score: 763, e: 0.24, Reward: -21.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-82610a883c3c>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mnext_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#print(next_frame.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mnext_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnext_frame\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Append new frame to state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m#print(next_state.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-3a39b8440924>\u001b[0m in \u001b[0;36mpreprocess_frame\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#print(type(frame))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mframe_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mframe_tensor\u001b[0m \u001b[0;31m#frame.numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.save(f\"dqn_model_final.pth\")"
      ],
      "metadata": {
        "id": "4Vkuk5NAd8_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('ALE/Pong-v5',render_mode='rgb_array')\n",
        "input_shape = (1, 84, 84)  # 4 frames, 84x84 each\n",
        "action_size = env.action_space.n\n",
        "agent = DQNAgent(input_shape, action_size)\n",
        "agent.load('dqn_model_final.pth')  # Load the trained model\n",
        "\n",
        "for episode in range(10):  # Play 10 episodes to test\n",
        "    observation = env.reset()\n",
        "    state = np.stack([agent.preprocess_frame(observation[0])] * 4, axis=0)  # Initialize state with 4 frames\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        env.render()\n",
        "        action = agent.act(state)\n",
        "        next_observation, reward, done, info, _ = env.step(action)\n",
        "        next_frame = agent.preprocess_frame(next_observation)\n",
        "        state = np.append(state[1:], [next_frame], axis=0)  # Append new frame to state\n",
        "        total_reward += reward\n",
        "    print(f\"Episode {episode + 1}: Total Reward: {total_reward}\")\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvdFLKtyn2Nd",
        "outputId": "1dce5d19-d82d-4a38-8c1e-e798bc140b53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-3a39b8440924>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  frame_tensor = torch.tensor(frame, dtype=torch.float32) / 255.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: Total Reward: -20.0\n",
            "Episode 2: Total Reward: -20.0\n",
            "Episode 3: Total Reward: -21.0\n",
            "Episode 4: Total Reward: -20.0\n",
            "Episode 5: Total Reward: -20.0\n",
            "Episode 6: Total Reward: -21.0\n",
            "Episode 7: Total Reward: -20.0\n",
            "Episode 8: Total Reward: -21.0\n",
            "Episode 9: Total Reward: -21.0\n",
            "Episode 10: Total Reward: -20.0\n"
          ]
        }
      ]
    }
  ]
}